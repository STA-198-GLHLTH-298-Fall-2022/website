<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Logistic Regression</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="css/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="css/slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Logistic Regression
## <br><br> Introduction to Global Health Data Science
### <br> Prof. Amy Herring

---





layout: true
  
&lt;div class="my-footer"&gt;
&lt;span&gt;
&lt;a href="https://sta-198-glhlth-298-fall-2022.github.io/website/" target="_blank"&gt;Back to website&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt; 

---










class: middle

# Logistic Regression

---

### Data

We consider data from the Global Adult Tobacco Survey (GATS), which is designed to provide nationally-representative data on non-institutionalized people 15 years and older. This survey is a global standard for systematically monitoring adult tobacco use and is produced by the Centers for Disease Control (CDC) in collaboration with the World Health Organization (WHO), RTI International, and Johns Hopkins University.

China has the largest smoking population in the world and accounts for roughly 40% of tobacco consumption worldwide. We will focus on GATS data from China in 2018 (the most recent survey year), but note data from other countries are available from the [WHO's Microdata Repository](https://extranet.who.int/ncdsmicrodata/index.php/home).

---


```r
gats &lt;- readr::read_csv("data/gats_rev.csv")

gats$RESIDENCE=factor(gats$RESIDENCE,levels=c(1,2),labels=c("Urban","Rural"))
gats$PROVINCE=factor(gats$PROVINCE,levels=seq(1:31),labels=c("Beijing","Tianjin","Hebei","Shanxi","Neimenggu","Liaoning","Jilin","Heilongjiang","Shanghai","Jiangsu","Zhejiang","Anhui","Fujian","Jiangxi","Shangdong","Henan","Hubei","Hunan","Guangdong","Guangxi","Hainan","Chongqing","Sichuan","Guizhou","Yunnan","Xizang","Shaanxi","Gansu","Qinghai","Ningxia","Xinjiang"))
gats$REGION6=factor(gats$REGION6,levels=seq(1:6),labels=c("North","North-East","East","South-Central","South-West","North-West"))
gats$REGION3=factor(gats$REGION3,levels=seq(1:3),labels=c("East","Central","West"))
gats$GENDER=factor(gats$GENDER,levels=c(1,2),labels=c("Male","Female"))
gats$CURRENTSMOKE=factor(gats$CURRENTSMOKE,levels=c(1,2,7),labels=c("Yes","No","Don't Know"))
gats$EDUCATION=factor(gats$EDUCATION,levels=c(1,2,3,4,5,6,7,8,77,99),labels=c("None","Less than Primary School","Primary School","Less than Secondary School","Secondary School","High School","University","Postgraduate","Don't Know","Refused"))
gats$OCCUPATION=factor(gats$OCCUPATION,levels=c(1,2,3,4,5,6,7,8,9,10,77,99),labels=c("Farming","Government","Business","Teacher","Healthcare","Student","Soldier","Unemployed","Retired","Other","Don't Know","Refused"))
gats$HEARDOFECIG=factor(gats$HEARDOFECIG,levels=c(1,2,9),labels=c("Yes","No","Refused"))
gats$ECIGUSE=factor(gats$ECIGUSE,levels=c(1,2,3,9),labels=c("Daily","Less than Daily","Not at All","Refused"))
gats$TRYSTOP=factor(gats$TRYSTOP,levels=c(1,2,9),labels=c("Yes","No","Refused"))
gats$HOMESMOKERULES=factor(gats$HOMESMOKERULES,levels=c(1,2,3,4,7,9),labels=c("Allowed","Not Allowed but Exceptions","Never Allowed","No Rules","Don't Know","Refused"))
gats$SMOKESICK=factor(gats$SMOKESICK,levels=c(1,2,7,9),labels=c("Yes","No","Don't Know","Refused"))
gats$SMOKESICKBINARY=ifelse(gats$SMOKESICK=="Yes","Yes","No")
gats$SMOKECANCER=factor(gats$SMOKECANCER,levels=c(1,2,7,9),labels=c("Yes","No","Don't Know","Refused"))
#scale age into decades
gats$DECADES=gats$AGE/10
```

What's up with all that code? I don't want alphabetical order in plots of education or other somewhat ordered categorical variables, so I'm using the levels and labels commands to specify ordering in advance.

---

### Selected variables

&lt;br&gt;

.midi[
variable        | description
----------------|-------------
`CURRENTSMOKE`      |	yes, no, or don't know
`AGE` |	computed from date of birth (`DECADES`=`AGE`/10)
`EDUCATION`	          | highest level of education completed
`OCCUPATION` | occupation
`RESIDENCE` | urban or rural
`GENDER`	          | interviewer instructions were "Record gender from observation. Ask if necessary"; options for male, female, missing/NA
`SMOKESICKYES` | respondent believes cigarette smoking can make you sick
]

Other data are also available in the file. Sample survey weights are not included but should be used to obtain nationally-representative estimates (our estimates are fairly close for the quantities we consider today).
---


## Is Believing Smoking Makes You Sick Related to Smoking?

.panelset[
.panel[.panel-name[Plot]
&lt;img src="w11-l02-logistic_files/figure-html/unnamed-chunk-2-1.png" width="50%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(gats, aes(x = SMOKESICKBINARY,
                 fill =CURRENTSMOKE)) +
  geom_bar() + 
  labs(x = 'Smoking Makes You Sick?',
       fill = 'Current Smoking') +
   scale_fill_manual(values = c("#E48957", "#071381","#07813e"))
```
]
]

---

## Is Gender Related to Smoking?

.panelset[
.panel[.panel-name[Plot]
&lt;img src="w11-l02-logistic_files/figure-html/unnamed-chunk-3-1.png" width="50%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(gats, aes(x = GENDER,
                 fill =CURRENTSMOKE)) +
  geom_bar() + 
  labs(x = 'Gender',
       fill = "Current Smoking") +
   scale_fill_manual(values = c("#E48957", "#071381","#07813e"))
```
]
]

---

## Is Education Related to Smoking?

.panelset[
.panel[.panel-name[Plot]
&lt;img src="w11-l02-logistic_files/figure-html/unnamed-chunk-4-1.png" width="50%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(gats, aes(x = EDUCATION,
                 fill = CURRENTSMOKE)) +
  geom_bar() +
  labs(x = 'Education',
       fill = "Current Smoking") +
  scale_fill_manual(values = c("#E48957", "#071381", "#07813e")) +
  theme(axis.text.x = element_text(angle = 45,hjust=1))
```
]
]

---

## Is Residence Related to Smoking?

.panelset[
.panel[.panel-name[Plot]
&lt;img src="w11-l02-logistic_files/figure-html/unnamed-chunk-5-1.png" width="50%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(gats, aes(x = RESIDENCE,
                 fill = CURRENTSMOKE)) +
  geom_bar() +
  labs(x = 'Residence',
       fill = "Current Smoking") +
  scale_fill_manual(values = c("#E48957", "#071381", "#07813e")) 
```
]
]

---

## Is Occupation Related to Smoking?

.panelset[
.panel[.panel-name[Plot]
&lt;img src="w11-l02-logistic_files/figure-html/unnamed-chunk-6-1.png" width="50%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(gats, aes(x = OCCUPATION,
                 fill = CURRENTSMOKE)) +
  geom_bar() +
  labs(x = 'OCCUPATION',
       fill = "Current Smoking") +
  scale_fill_manual(values = c("#E48957", "#071381", "#07813e")) +
  theme(axis.text.x = element_text(angle = 45,hjust=1))
```
]
]

---

## Is Age Related to Smoking?

.panelset[
.panel[.panel-name[Plot]
&lt;img src="w11-l02-logistic_files/figure-html/unnamed-chunk-7-1.png" width="50%" style="display: block; margin: auto;" /&gt;
]


.panel[.panel-name[Code]

```r
library(ggridges)
gats %&gt;%
  ggplot(aes(x = AGE, y = CURRENTSMOKE, fill = CURRENTSMOKE, color = CURRENTSMOKE)) +
  geom_density_ridges2(alpha = 0.5) +
  labs(
    x = "Age (years)", 
    y = "Current Smoking",
    title = "Age vs. Current Smoking"
    ) +
  guides(color = FALSE, fill = FALSE) +
  scale_fill_manual(values = c("#E48957", "#071381", "#07813e")) +
  scale_color_manual(values = c("#E48957", "#071381", "#07813e")) 
```
]
]

---

## Modelling smoking

- Age, gender, education, occupation, residence, and beliefs about effects of smoking might all be related to whether a person smokes. How do we come up with a model that will let us explore this relationship?

--
- For simplicity, we'll focus on age (in decades) (`DECADES`) as predictor, but the model we describe can be expanded to take multiple predictors as well.

---

## Don't Know What to Do About Don't Know

The "don't know" for current smoking category is complicated. Ideally, we would work with the survey leaders to understand whether "don't know" is a way to avoid saying "yes," an indicator of only occasional smoking, or something else. For now, we will drop levels of "don't know" or "refused" throughout and just consider responses of people who provided substantive answers to questions.


```r
gatsbin &lt;- gats %&gt;%
  filter(CURRENTSMOKE != "Don't Know") %&gt;%
  filter(EDUCATION != "Don't Know" &amp; EDUCATION != "Refused") %&gt;%
  filter(OCCUPATION != "Don't Know" &amp; OCCUPATION != "Refused")

#I used these two lines of code to get
#rid of the empty "Don't Know" level
gatsbin$SMOKE &lt;- gatsbin$CURRENTSMOKE[gatsbin$CURRENTSMOKE != "Don't Know"]
gatsbin$SMOKE &lt;- factor(gatsbin$SMOKE)


gatsbin$SMOKE &lt;- relevel(gatsbin$SMOKE, ref = "No")
```

---




## Modelling Smoking

This isn't something we can reasonably fit a linear model to -- we need something different!

&lt;img src="w11-l02-logistic_files/figure-html/unnamed-chunk-8-1.png" width="70%" style="display: block; margin: auto;" /&gt;



---

## Framing the problem

- We can treat each outcome (smoking and not) as successes and failures arising from separate Bernoulli trials
  - Bernoulli trial: a random experiment with exactly two possible outcomes, "success" and "failure", in which the probability of success *is the same* every time the experiment is conducted

--
- We can generalize by allowing each Bernoulli trial to have a separate probability of success. We can state that as `\(P(y_i=1)=\pi_i\)` or as `\(y_i ∼ Bern(\pi_i)\)`, where the tilde is read "is distributed as".

--
- We can then use the predictor variables to model that probability of success, `\(\pi_i\)`. For example, we know `\(\pi_i\)` is much higher for men in this study than for women. We could further refine `\(\pi_i\)` by letting it be different for those who believe smoking makes you sick and those who don't, etc.

--
- We can't just use a linear regression model for `\(\pi_i\)` ($\pi_i$ must be between 0 and 1), but we can transform the linear model to have the appropriate range

---

## Generalized linear models

- This is a very general way of addressing many problems in regression and the resulting models are called **generalized linear models (GLMs)**

--
- Logistic regression is just one example

---

## Three characteristics of GLMs

All GLMs have the following three characteristics:

`1` A probability distribution describing a generative model for the outcome variable. For binary outcomes like yes/no smoking, a Bernoulli distribution makes sense. For continuous outcomes like BMI, a normal distribution is often appropriate.

--

`2` A linear model:
`\(\eta_i = \beta_0 + \beta_1 X_{1,i} + \cdots + \beta_k X_{k,i}\)`

We have already studied linear regression, where `\(y_i=\eta_i+\varepsilon_i\)` or `\(y_i=\beta_0 + \beta_1 X_{1,i} + \cdots + \beta_k X_{k,i}+\varepsilon_i\)`

Our predictors `\(X_i\)` can be binary, continuous, etc. For example, maybe `\(X_{1,i}\)` takes value 1 for one gender and 0 for the other, and maybe `\(X_{k,i}\)` is age in years. Here, `\(k\)` is just an generic index indicating we could have any number `\(k\)` of predictors.

---

## Three characteristics of GLMs


`3` A link function that relates the linear model to the parameter of the outcome distribution. In linear regression, we have the **identity** link. This is a nice case, where our response `\(y_i\)` is continuous and can theoretically range from `\(-\infty\)` to `\(\infty\)`. 

If our response is binary, we need to be sure that our predicted `\(\pi_i\)` is a valid probability ranging from 0 to 1, so a form like `$$\pi_i=\beta_0 + \beta_1 X_{1,i} + \cdots + \beta_k X_{k,i}$$` will not work because if our `\(X\)`'s are positive but our estimates `\(\hat{\beta}\)`'s are negative, then `\(\hat{\pi_i}\)` will be negative and not a valid probability.

The **link function** is used to map our **linear predictor** `\(\eta_i=\beta_0 + \beta_1 X_{1,i} + \cdots + \beta_k X_{k,i}\)` to an appropriate range (e.g., 0-1 if we are modeling a probability).
  
---

## A Familiar GLM: The Normal Linear Model

We can formulate the linear regression model as a GLM!

  - Probability distribution: `\(y_i \sim N(\mu_i,\sigma^2)\)`
  
  - `\(\eta_i=\beta_0 + \beta_1 X_{1,i} + \cdots + \beta_k X_{k,i}\)`
  
  - Link function: `\(\mu_i=\eta_i\)`
  
  
We can put it all together as `$$y_i=\beta_0 + \beta_1 X_{1,i} + \cdots + \beta_k X_{k,i} + \varepsilon_i$$` where `\(\varepsilon_i \sim N\left(0,\sigma^2\right)\)`



---

## Nice Features of the GLM

The GLM provides an overarching framework for estimation and testing in a broad class of models. For any GLM, the following apply.

  - A hypothesis test of `\(H_0: \beta=0\)` versus `\(H_A: \beta \neq 0\)` can be obtained using a z-statistic. In linear regression, an "exact" test (t-test) is available if the response is normally distributed, and for this reason the t-test is the default for linear regression. In other GLM's, the z-tests are approximate and require a "big enough" sample size.
  
  - 95% confidence intervals for `\(\beta\)` can be constructed as `\(\hat{\beta} \pm 1.96 \times s_\hat{\beta}\)` - again these are valid as long as we have "enough" data -- and for other coverage levels (e.g., 99% CI), we just use the relevant confidence multiplier. For linear regression, t-intervals are typically used (this changes the confidence multiplier) because they have better properties if the sample size is not that large.
  
If we reject `\(H_0\)`, or if our confidence interval excludes the value 0, our conclusion is that our data provide evidence against the null hypothesis.


---

class: middle

# Logistic regression

---

## Logistic regression

- Logistic regression is a GLM used to model a binary categorical outcome using numerical and categorical predictors

- Here our probability distribution is the Bernoulli, and we can define `\(\eta_i\)` as we do in linear regression

--
- To finish specifying the logistic model we just need to define a reasonable link function that connects `\(\eta_i\)` to `\(\pi_i\)`: logit function

--
- **Logit function:** For `\(0\le \pi \le 1\)`

`$$logit(\pi) = \log\left(\frac{\pi}{1-\pi}\right)$$`
`\(\frac{\pi}{1-\pi}\)` is the odds of the response.


---

## Logit function, visualised

&lt;img src="w11-l02-logistic_files/figure-html/unnamed-chunk-9-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

## Properties of the logit

- The logit function takes a value between 0 and 1 and maps it to a value between `\(-\infty\)` and `\(\infty\)`

--
- Inverse logit (logistic) function:
`$$g^{-1}(x) = \frac{\exp(x)}{1+\exp(x)} = \frac{1}{1+\exp(-x)}$$`

--
- The inverse logit function takes a value between `\(-\infty\)` and `\(\infty\)` and maps it to a value between 0 and 1

--
- This formulation is also useful for interpreting the model, since the logit can be interpreted as the log odds of a success -- more on this later

---

## The logistic regression model

- Based on the three GLM criteria we have
  - `\(y_i \sim \text{Bern}(\pi_i)\)`
  - `\(\eta_i = \beta_0+\beta_1 x_{1,i} + \cdots + \beta_n x_{n,i}\)`
  - `\(\text{logit}(\pi_i) = \eta_i\)`

--
- From which we get

`$$\pi_i = \frac{\exp(\beta_0+\beta_1 x_{1,i} + \cdots + \beta_k x_{k,i})}{1+\exp(\beta_0+\beta_1 x_{1,i} + \cdots + \beta_k x_{k,i})}$$`
--

What value of `\(\beta\)` here would indicate our predictor variable is not related to our response?

---

## Odds Ratios

Consider a basic model:

`$$logit(\pi_i) = \log\left(\frac{\pi_i}{1-\pi_i}\right)=\beta_0+\beta_1x_{1,i}+\beta_2x_{2,i},$$`
which is equivalent to 

`$$\left(\frac{\pi_i}{1-\pi_i}\right)=\exp\left(\beta_0+\beta_1x_{1,i}+\beta_2x_{2,i}\right)$$`

This is a multiplicative model for the odds. For example, if
we change `\(x_{1,i}\)` by one unit while holding the other variable `\(x_{2,i}\)` constant, we multiply the odds by `\(\exp(\beta_1)\)` because `$$\exp(\beta_1(x_{1,i}+1))=\exp(\beta_1x_{1,i})\exp(\beta_1).$$`

---

`$$logit(\pi_i) = \log\left(\frac{\pi_i}{1-\pi_i}\right)=\beta_0+\beta_1x_{1,i}+\beta_2x_{2,i}$$`
`$$\exp(\beta_1(x_{1,i}+1))=\exp(\beta_1x_{1,i})\exp(\beta_1)$$`

Thus we can interpret `\(\exp(\beta_1)\)` as the **odds ratio (OR)** for the response (here, smoking), comparing people with value `\(x_{1,i}+1\)` to value `\(x_{1,i}\)` (a one-unit difference).


Note that in a logistic regression model, negative logits represent probabilities less than one-half, and
positive logits represent probabilities above one-half.

---


## Modeling smoking

In R we fit a GLM in the same way as a linear model except we

- specify the model with `logistic_reg()`
- use `"glm"` instead of `"lm"` as the engine 
- define `family = "binomial"` for the link function to be used in the model

--

Our model is `$$\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_0+\beta_1\times \text{DECADES_i},$$` where `\(\pi_i\)` is the probability person `\(i\)` is a smoker. If age and smoking are unrelated, we would expect to see `\(\beta_1\)` close to 0.
---


```r
#using DECADES to get age scaled by 10 year intervals
smoke_fit &lt;- logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  fit(SMOKE ~ DECADES, data = gatsbin, family = "binomial")

tidy(smoke_fit)
```

```
#&gt; # A tibble: 2 × 5
#&gt;   term        estimate std.error statistic  p.value
#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
#&gt; 1 (Intercept)  -1.19      0.0569    -20.9  1.26e-96
#&gt; 2 DECADES       0.0234    0.0103      2.27 2.30e- 2
```


The value `statistic` is calculated as `estimate` minus 0 (the hypothesized value) divided by `std.error`, and the p-value is obtained by comparing the `statistic` to a standard normal distribution.

---

## Smoking model


```r
tidy(smoke_fit, conf.int=TRUE)
```

```
#&gt; # A tibble: 2 × 7
#&gt;   term        estimate std.er…¹ stati…²  p.value conf.low conf.…³
#&gt;   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;
#&gt; 1 (Intercept)  -1.19     0.0569  -20.9  1.26e-96 -1.30    -1.07  
#&gt; 2 DECADES       0.0234   0.0103    2.27 2.30e- 2  0.00324  0.0436
#&gt; # … with abbreviated variable names ¹​std.error, ²​statistic,
#&gt; #   ³​conf.high
```

--
Fitted model:
`$$\log\left(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\right) = -1.19+0.0234\times \text{DECADES_i}$$`
Our confidence interval does not contain 0, so we can reject the null hypothesis that age is unrelated to smoking.

---

We can interpret the estimate `\(\hat{\beta}_1=0.0234\)` as the log OR for a one-unit increase in decades: the odds of smoking for a group one decade older is `\(\exp(0.0234)\)` = 1.024 times the odds of the referent group.  Alternatively, each 10 year increment in age is associated with  1.024 times the odds of smoking. Older individuals have higher odds of smoking.  

In a *multiple logistic regression model* with many predictors, we'd interpret this OR conditionally on fixed values of the other predictors, just as we would in multiple linear regression.

---

## Hypothesis Tests in Logistic Regression

Generally, we wish to know whether the OR=1 or equivalently whether the log OR (a `\(\beta\)` coefficient)=0.  To test `\(H_0:\beta_j=0\)`, we can compare the ratio of a parameter estimate to its standard error using `$$z=\frac{\hat{\beta}_j-0}{s_{\hat{\beta}_j}},$$` comparing this z-statistic to the standard normal distribution.  


Confidence intervals for the effects on the logit scale, `\(\hat{\beta}_j \pm 1.96s_{\hat{\beta}_j}\)`, are typically translated into confidence intervals for OR's by exponentiating the lower and upper confidence limits as `\(\left(\exp^{\hat{\beta}_j - 1.96s_{\hat{\beta}_j}}, \exp^{\hat{\beta}_j + 1.96s_{\hat{\beta}_j}}\right)\)` .

---





## P(smoking) for a person aged 20

Next, we'll back out estimates of probabilities from our model.


`$$\log \frac{\hat{\pi}_i}{1-\hat{\pi}_i} = -1.19+0.0234\times 2$$`
--
`$$\frac{\hat{\pi}_i}{1-\hat{\pi}_i} = \exp(-1.1432) = 0.3188 \rightarrow \hat{\pi} = 0.3188 \times (1 - \hat{\pi})$$`

--
`$$\hat{\pi} = 0.3188 - 0.3188\hat{\pi} \rightarrow 1.3188\hat{\pi} = 0.3188$$`
--
`$$\hat{\pi} = 0.3188 / 1.3188 = 0.24$$`

---

.question[
What is the probability that a person aged 30 smokes? What about a person aged 70?
]

--


.pull-left[
&lt;img src="w11-l02-logistic_files/figure-html/smoke-predict-viz-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
.pull-right[
- .light-blue[20 years: P(smoke) = 0.24]
- .yellow[30 years, P(smoke) = 0.25]
- .green[70 years, P(smoke) = 0.26]
]


---

## Multiple Logistic Regression

Let's look at the relationships now among age, gender, occupation, and current smoking.  First, let's go back to the plots.

&lt;img src="w11-l02-logistic_files/figure-html/replot-1.png" width="50%" style="display: block; margin: auto;" /&gt;

The gender effect seems strong. Male will be the referent group. You can check using the code `levels(gats$GENDER)` -- the referent will be the first level.

---

Here are the levels of occupation (don't know and refused have been excluded).

&lt;img src="w11-l02-logistic_files/figure-html/occbin-1.png" width="50%" style="display: block; margin: auto;" /&gt;

The default referent group in the model will be the first level shown here, farming, and the other occupation levels will be represented by 1/0 indicator variables. Recall occupation is a factor variable, and each person belongs to just one level of occupation.

---

Our model is `$$\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_0+\beta_1\times \text{DECADES}_i + \beta_2 \times \text{Female}_i + \beta_3 \times \text{Government}_i$$` `$$+ ~\beta_4 \times \text{Business}_i + \beta_5 \times \text{Teacher}_i + \beta_6 \times \text{Healthcare}_i+\beta_7 \times \text{Student}_i$$` `$$+ ~\beta_8 \times \text{Soldier}_i + \beta_9 \times \text{Unemployed}_i+ \beta_{10} \times \text{Retired}_i + \beta_{11} \times \text{Other}_i,$$` where `\(\pi_i\)` is the probability person `\(i\)` is a smoker.


The way the indicator variables work is that `\(\text{Business}_i=1\)` if the occupation of person `\(i\)` is business and 0 if the occupation is not business. The other indicator variables work the same way, and we know a person is a farmer if `$$\text{Government}_i=\text{Business}_i=\text{Teacher}_i=\text{Healthcare}_i=\text{Student}_i=\text{Soldier}_i$$`
$$ = \text{Unemployed}_i=\text{Retired}_i=\text{Other}_i=0$$.
---

Let's fit the model!

.panelset[
.panel[.panel-name[Output]

```
#&gt; # A tibble: 12 × 7
#&gt;    term         estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.h…⁵
#&gt;    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
#&gt;  1 (Intercept)   0.443   0.0973   4.56  5.19e- 6  0.253   0.634  
#&gt;  2 DECADES      -0.0227  0.0157  -1.44  1.49e- 1 -0.0535  0.00815
#&gt;  3 GENDERFemale -3.81    0.0685 -55.7   0        -3.95   -3.68   
#&gt;  4 OCCUPATIONG… -0.547   0.116   -4.73  2.20e- 6 -0.775  -0.321  
#&gt;  5 OCCUPATIONB… -0.264   0.0594  -4.45  8.71e- 6 -0.381  -0.148  
#&gt;  6 OCCUPATIONT… -0.846   0.185   -4.58  4.68e- 6 -1.22   -0.490  
#&gt;  7 OCCUPATIONH… -0.699   0.204   -3.43  5.99e- 4 -1.11   -0.305  
#&gt;  8 OCCUPATIONS… -2.68    0.280   -9.58  9.58e-22 -3.27   -2.17   
#&gt;  9 OCCUPATIONS… -1.36    0.586   -2.32  2.02e- 2 -2.65   -0.283  
#&gt; 10 OCCUPATIONU… -0.328   0.0805  -4.07  4.64e- 5 -0.486  -0.171  
#&gt; 11 OCCUPATIONR… -0.624   0.0691  -9.04  1.56e-19 -0.760  -0.489  
#&gt; 12 OCCUPATIONO… -0.0393  0.0639  -0.614 5.39e- 1 -0.165   0.0861 
#&gt; # … with abbreviated variable names ¹​estimate, ²​std.error,
#&gt; #   ³​statistic, ⁴​conf.low, ⁵​conf.high
```
]
.panel[.panel-name[Code]

```r
smoke_fit_multi &lt;- logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  fit(SMOKE ~ DECADES + GENDER + OCCUPATION, data=gatsbin, family="binomial")
result&lt;-tidy(smoke_fit_multi, conf.int=TRUE)
print(result,n=14)
```

```
#&gt; # A tibble: 12 × 7
#&gt;    term         estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.h…⁵
#&gt;    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
#&gt;  1 (Intercept)   0.443   0.0973   4.56  5.19e- 6  0.253   0.634  
#&gt;  2 DECADES      -0.0227  0.0157  -1.44  1.49e- 1 -0.0535  0.00815
#&gt;  3 GENDERFemale -3.81    0.0685 -55.7   0        -3.95   -3.68   
#&gt;  4 OCCUPATIONG… -0.547   0.116   -4.73  2.20e- 6 -0.775  -0.321  
#&gt;  5 OCCUPATIONB… -0.264   0.0594  -4.45  8.71e- 6 -0.381  -0.148  
#&gt;  6 OCCUPATIONT… -0.846   0.185   -4.58  4.68e- 6 -1.22   -0.490  
#&gt;  7 OCCUPATIONH… -0.699   0.204   -3.43  5.99e- 4 -1.11   -0.305  
#&gt;  8 OCCUPATIONS… -2.68    0.280   -9.58  9.58e-22 -3.27   -2.17   
#&gt;  9 OCCUPATIONS… -1.36    0.586   -2.32  2.02e- 2 -2.65   -0.283  
#&gt; 10 OCCUPATIONU… -0.328   0.0805  -4.07  4.64e- 5 -0.486  -0.171  
#&gt; 11 OCCUPATIONR… -0.624   0.0691  -9.04  1.56e-19 -0.760  -0.489  
#&gt; 12 OCCUPATIONO… -0.0393  0.0639  -0.614 5.39e- 1 -0.165   0.0861 
#&gt; # … with abbreviated variable names ¹​estimate, ²​std.error,
#&gt; #   ³​statistic, ⁴​conf.low, ⁵​conf.high
```
]
]


---

Let's print the levels of occupation so we can interpret those terms. Note the (empty) levels for "don't know" and "refused" still are printed, even though we dropped them using the `filter` command.


```r
levels(gats$OCCUPATION)
```

```
#&gt;  [1] "Farming"    "Government" "Business"   "Teacher"   
#&gt;  [5] "Healthcare" "Student"    "Soldier"    "Unemployed"
#&gt;  [9] "Retired"    "Other"      "Don't Know" "Refused"
```

---

Let's exponentiate so we have our OR's.  

.panelset[
.panel[.panel-name[Output]

```
#&gt; # A tibble: 12 × 7
#&gt;    term          estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.…⁵
#&gt;    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 (Intercept)    1.56    0.0973   4.56  5.19e- 6  1.29    1.89  
#&gt;  2 DECADES        0.978   0.0157  -1.44  1.49e- 1  0.948   1.01  
#&gt;  3 GENDERFemale   0.0221  0.0685 -55.7   0         0.0193  0.0252
#&gt;  4 OCCUPATIONGo…  0.579   0.116   -4.73  2.20e- 6  0.461   0.725 
#&gt;  5 OCCUPATIONBu…  0.768   0.0594  -4.45  8.71e- 6  0.683   0.863 
#&gt;  6 OCCUPATIONTe…  0.429   0.185   -4.58  4.68e- 6  0.296   0.613 
#&gt;  7 OCCUPATIONHe…  0.497   0.204   -3.43  5.99e- 4  0.331   0.737 
#&gt;  8 OCCUPATIONSt…  0.0684  0.280   -9.58  9.58e-22  0.0380  0.115 
#&gt;  9 OCCUPATIONSo…  0.257   0.586   -2.32  2.02e- 2  0.0709  0.753 
#&gt; 10 OCCUPATIONUn…  0.720   0.0805  -4.07  4.64e- 5  0.615   0.843 
#&gt; 11 OCCUPATIONRe…  0.536   0.0691  -9.04  1.56e-19  0.468   0.613 
#&gt; 12 OCCUPATIONOt…  0.961   0.0639  -0.614 5.39e- 1  0.848   1.09  
#&gt; # … with abbreviated variable names ¹​estimate, ²​std.error,
#&gt; #   ³​statistic, ⁴​conf.low, ⁵​conf.high
```
]
.panel[.panel-name[Code]

```r
result2&lt;-tidy(smoke_fit_multi, conf.int=TRUE, exponentiate=TRUE)
print(result2,n=14)
```

```
#&gt; # A tibble: 12 × 7
#&gt;    term          estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.…⁵
#&gt;    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 (Intercept)    1.56    0.0973   4.56  5.19e- 6  1.29    1.89  
#&gt;  2 DECADES        0.978   0.0157  -1.44  1.49e- 1  0.948   1.01  
#&gt;  3 GENDERFemale   0.0221  0.0685 -55.7   0         0.0193  0.0252
#&gt;  4 OCCUPATIONGo…  0.579   0.116   -4.73  2.20e- 6  0.461   0.725 
#&gt;  5 OCCUPATIONBu…  0.768   0.0594  -4.45  8.71e- 6  0.683   0.863 
#&gt;  6 OCCUPATIONTe…  0.429   0.185   -4.58  4.68e- 6  0.296   0.613 
#&gt;  7 OCCUPATIONHe…  0.497   0.204   -3.43  5.99e- 4  0.331   0.737 
#&gt;  8 OCCUPATIONSt…  0.0684  0.280   -9.58  9.58e-22  0.0380  0.115 
#&gt;  9 OCCUPATIONSo…  0.257   0.586   -2.32  2.02e- 2  0.0709  0.753 
#&gt; 10 OCCUPATIONUn…  0.720   0.0805  -4.07  4.64e- 5  0.615   0.843 
#&gt; 11 OCCUPATIONRe…  0.536   0.0691  -9.04  1.56e-19  0.468   0.613 
#&gt; 12 OCCUPATIONOt…  0.961   0.0639  -0.614 5.39e- 1  0.848   1.09  
#&gt; # … with abbreviated variable names ¹​estimate, ²​std.error,
#&gt; #   ³​statistic, ⁴​conf.low, ⁵​conf.high
```
]
]


---

## Interpreting the Results

First, take a look at our estimate for `\(\beta_1\)` or `\(\exp(\beta_1)\)`, the coefficient of DECADES. This is no longer significant and in fact even goes in the opposite direction!  So conditional on gender and occupation, age is no longer related to the probability of smoking. 

Gender is a very strong predictor of smoking. Conditional on age and occupation, those identifying as female in the data have just 0.022 (95% CI=(0.019,0.025)) times the odds of smoking as those not identifying as female.

---

There are many occupation estimates to interpret!  First, let's pick off the easy one -- conditional on gender and age, we don't see evidence that the odds of smoking are any different for those who reported other occupations than they are for farmers.  Conditional on age and gender, all the remaining occupations have lower odds of smoking than do farmers.  For example, those in government positions have 0.58 (95% CI=(0.46, 0.73)) times the odds of smoking as farmers, conditional on gender and age. Those in healthcare have 0.50 (95% CI=(0.33, 0.74)) times the odds of smoking as farmers, conditional on gender and age.

---

One issue with the age term is that we may wish to stratify by gender in examining its effect -- for example maybe there's just an association between smoking and age within gender. It's good to explore this because gender is such a strong predictor, with few women smoking.  So let's add an interaction between gender and smoking to the model.

---


```r
smoke_int &lt;- logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  fit(SMOKE ~ DECADES+GENDER + DECADES*GENDER + OCCUPATION, data=gatsbin, family="binomial")
result_int&lt;-tidy(smoke_int, conf.int=TRUE,exponentiate=TRUE)
print(result_int,n=15)
```

```
#&gt; # A tibble: 13 × 7
#&gt;    term          estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.…⁵
#&gt;    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 (Intercept)   1.87     0.101    6.17  6.91e-10 1.53    2.28   
#&gt;  2 DECADES       0.945    0.0165  -3.40  6.82e- 4 0.915   0.977  
#&gt;  3 GENDERFemale  0.00421  0.269  -20.3   4.71e-92 0.00246 0.00707
#&gt;  4 OCCUPATIONGo… 0.570    0.116   -4.83  1.33e- 6 0.453   0.715  
#&gt;  5 OCCUPATIONBu… 0.758    0.0600  -4.61  3.99e- 6 0.674   0.853  
#&gt;  6 OCCUPATIONTe… 0.437    0.187   -4.42  9.93e- 6 0.301   0.628  
#&gt;  7 OCCUPATIONHe… 0.501    0.207   -3.34  8.47e- 4 0.332   0.749  
#&gt;  8 OCCUPATIONSt… 0.0617   0.281   -9.92  3.58e-23 0.0342  0.104  
#&gt;  9 OCCUPATIONSo… 0.246    0.586   -2.39  1.67e- 2 0.0680  0.723  
#&gt; 10 OCCUPATIONUn… 0.725    0.0804  -4.01  6.11e- 5 0.619   0.848  
#&gt; 11 OCCUPATIONRe… 0.547    0.0684  -8.83  1.05e-18 0.478   0.625  
#&gt; 12 OCCUPATIONOt… 0.951    0.0640  -0.787 4.31e- 1 0.839   1.08   
#&gt; 13 DECADES:GEND… 1.35     0.0448   6.69  2.18e-11 1.24    1.47   
#&gt; # … with abbreviated variable names ¹​estimate, ²​std.error,
#&gt; #   ³​statistic, ⁴​conf.low, ⁵​conf.high
```

---
The interaction term is listed last in this output and is constructed by multiplying the term `DECADES` by the indicator variable for female gender. 

Now, both main effects and the interaction term are statistically significant. This is a bit tricky to interpret, so let's write out the model separately for men and women, and for simplicity let's consider the occupation farming (the referent). Since there's no interaction involving occupation, it doesn't matter which occupation we pick for this comparison, as long as we hold it constant.

---

Our full model is
`$$\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_0+\beta_1\times \text{DECADES}_i + \beta_2 \times \text{Female}_i + \beta_3 \times \text{Government}_i$$` `$$+ ~\beta_4 \times \text{Business}_i + \beta_5 \times \text{Teacher}_i + \beta_6 \times \text{Healthcare}_i+\beta_7 \times \text{Student}_i$$` `$$+ ~\beta_8 \times \text{Soldier}_i + \beta_9 \times \text{Unemployed}_i+ \beta_{10} \times \text{Retired}_i + \beta_{11} \times \text{Other}_i$$` `$$+ ~\beta_{12} \times \text{Female}_i \times \text{DECADES}_i$$`
Model for male farmers: `\(\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_0+\beta_1\times \text{DECADES}_i\)`

Model for female farmers: `\(\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_0+\beta_1\times \text{DECADES}_i + \beta_2 \times \text{Female}_i + \beta_{12} \times \text{Female}_i \times \text{DECADES}_i\)`

---

Model for male farmers: `\(\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_0+\beta_1\times \text{DECADES}_i\)`

Model for female farmers: `\(\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_0+\beta_2 + (\beta_1 + \beta_{12}) \times \text{DECADES}_i\)`

So `\(\beta_1\)` gives is the age effect among men:  conditional on occupation, a one decade increase in age among men is associated with 0.95 (95% CI=(0.92, 0.98)) times the odds of smoking.

---

We have to be careful about getting the OR of age among females.  The **odds** of smoking for a female farmer are given by `$$\exp\left(\beta_0+\beta_2 + (\beta_1 + \beta_{12}) \times \text{DECADES}_i\right),$$` and if we add a decade, the odds of smoking for that female farmer one decade older are given by `$$\exp\left(\beta_0+\beta_2 + (\beta_1 + \beta_{12}) \times (\text{DECADES}_i+1)\right).$$` This means the OR for an increase in age (in decades) among females is given by `\(\frac{\exp\left(\beta_0+\beta_2 + (\beta_1 + \beta_{12}) \times (\text{DECADES}_i+1)\right)}{\exp\left(\beta_0+\beta_2 + (\beta_1 + \beta_{12}) \times \text{DECADES}_i\right)}=\exp\left(\beta_1+\beta_{12}\right).\)` We can get the estimated OR itself by just taking `\(\exp\left(\beta_1+\beta_{12}\right)=\exp(\beta_1)\exp(\beta_{12})=1.28,\)` but the variance of this quantity cannot be estimated from the model fitting output provided. This is a bit of a drag.

---

The quickest way to get the OR for age for females is just to change the referent level of gender and pull it off the main effect for age (like we did above for males). 


.panelset[
.panel[.panel-name[Output]

```
#&gt; # A tibble: 13 × 7
#&gt;    term          estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.…⁵
#&gt;    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 (Intercept)   7.85e-3  0.261  -18.6   5.22e-77 4.67e-3 1.30e-2
#&gt;  2 DECADES       1.28e+0  0.0429   5.69  1.28e- 8 1.17e+0 1.39e+0
#&gt;  3 GENDERMale    2.38e+2  0.269   20.3   4.71e-92 1.41e+2 4.06e+2
#&gt;  4 OCCUPATIONGo… 5.70e-1  0.116   -4.83  1.33e- 6 4.53e-1 7.15e-1
#&gt;  5 OCCUPATIONBu… 7.58e-1  0.0600  -4.61  3.99e- 6 6.74e-1 8.53e-1
#&gt;  6 OCCUPATIONTe… 4.37e-1  0.187   -4.42  9.93e- 6 3.01e-1 6.28e-1
#&gt;  7 OCCUPATIONHe… 5.01e-1  0.207   -3.34  8.47e- 4 3.32e-1 7.49e-1
#&gt;  8 OCCUPATIONSt… 6.17e-2  0.281   -9.92  3.58e-23 3.42e-2 1.04e-1
#&gt;  9 OCCUPATIONSo… 2.46e-1  0.586   -2.39  1.67e- 2 6.80e-2 7.23e-1
#&gt; 10 OCCUPATIONUn… 7.25e-1  0.0804  -4.01  6.11e- 5 6.19e-1 8.48e-1
#&gt; 11 OCCUPATIONRe… 5.47e-1  0.0684  -8.83  1.05e-18 4.78e-1 6.25e-1
#&gt; 12 OCCUPATIONOt… 9.51e-1  0.0640  -0.787 4.31e- 1 8.39e-1 1.08e+0
#&gt; 13 DECADES:GEND… 7.41e-1  0.0448  -6.69  2.18e-11 6.78e-1 8.09e-1
#&gt; # … with abbreviated variable names ¹​estimate, ²​std.error,
#&gt; #   ³​statistic, ⁴​conf.low, ⁵​conf.high
```
]
.panel[.panel-name[Code]

```r
gatsfem &lt;- gatsbin

gatsfem$GENDER=relevel(gatsfem$GENDER, ref = "Female")

smoke_int_femref &lt;- logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  fit(SMOKE ~ DECADES+GENDER + DECADES*GENDER + OCCUPATION, data=gatsfem, family="binomial")
result3&lt;-tidy(smoke_int_femref, conf.int=TRUE, exponentiate=TRUE)
print(result3,n=15)
```

```
#&gt; # A tibble: 13 × 7
#&gt;    term          estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.…⁵
#&gt;    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 (Intercept)   7.85e-3  0.261  -18.6   5.22e-77 4.67e-3 1.30e-2
#&gt;  2 DECADES       1.28e+0  0.0429   5.69  1.28e- 8 1.17e+0 1.39e+0
#&gt;  3 GENDERMale    2.38e+2  0.269   20.3   4.71e-92 1.41e+2 4.06e+2
#&gt;  4 OCCUPATIONGo… 5.70e-1  0.116   -4.83  1.33e- 6 4.53e-1 7.15e-1
#&gt;  5 OCCUPATIONBu… 7.58e-1  0.0600  -4.61  3.99e- 6 6.74e-1 8.53e-1
#&gt;  6 OCCUPATIONTe… 4.37e-1  0.187   -4.42  9.93e- 6 3.01e-1 6.28e-1
#&gt;  7 OCCUPATIONHe… 5.01e-1  0.207   -3.34  8.47e- 4 3.32e-1 7.49e-1
#&gt;  8 OCCUPATIONSt… 6.17e-2  0.281   -9.92  3.58e-23 3.42e-2 1.04e-1
#&gt;  9 OCCUPATIONSo… 2.46e-1  0.586   -2.39  1.67e- 2 6.80e-2 7.23e-1
#&gt; 10 OCCUPATIONUn… 7.25e-1  0.0804  -4.01  6.11e- 5 6.19e-1 8.48e-1
#&gt; 11 OCCUPATIONRe… 5.47e-1  0.0684  -8.83  1.05e-18 4.78e-1 6.25e-1
#&gt; 12 OCCUPATIONOt… 9.51e-1  0.0640  -0.787 4.31e- 1 8.39e-1 1.08e+0
#&gt; 13 DECADES:GEND… 7.41e-1  0.0448  -6.69  2.18e-11 6.78e-1 8.09e-1
#&gt; # … with abbreviated variable names ¹​estimate, ²​std.error,
#&gt; #   ³​statistic, ⁴​conf.low, ⁵​conf.high
```
]
]

---

There you have it, the estimated OR is 1.28, 95% CI=(1.17, 1.39), so holding occupation constant, a one-decade increase in age for women is associated with 1.28 times the odds of smoking.

What's up with male gender here?  Well, it wasn't as obvious in the prior model, but we had the same problem. So few women smoke in the data that these odds ratios are getting quite large (or small in the other model). It might even make more sense to exclude women entirely when examining predictors of smoking.

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLines": true,
"highlightStyle": "solarized-light",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
